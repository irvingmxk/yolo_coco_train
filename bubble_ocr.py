#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ°”æ³¡æ£€æµ‹ + OCR æ–‡å­—è¯†åˆ«ç³»ç»Ÿ
ä½¿ç”¨ YOLO11n æ£€æµ‹æ°”æ³¡æ¡†ï¼Œç„¶åä½¿ç”¨ OCR è¯†åˆ«æ¡†å†…æ–‡å­—
"""

import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from ultralytics import YOLO
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from chat_record_manager import chat_record_manager


# ============= é…ç½®å‚æ•° =============
CONFIG = {
    # YOLOæ¨¡å‹é…ç½®
    'yolo_model': 'runs/train/yolo11s_bubble/weights/best.pt',  # YOLOæ¨¡å‹è·¯å¾„
    # 'yolo_model': 'runs/train/bubble_detection/weights/best.pt',  # YOLO8sæ¨¡å‹è·¯å¾„

    # yolo_conf è®¾ç½®ä¸º 0.4~0.6ï¼Œå»ºè®® 0.5ï¼ˆæ£€æµ‹æ¡†è¾ƒå‡†ç¡®ä¸”ä¸è¿‡å¤šï¼‰ï¼Œå¦‚é‡è¿‡å¤šè¯¯æ£€å¯é€‚å½“æé«˜
    # yolo_iou 0.4~0.6ï¼Œ0.45 é€šå¸¸è¡¨ç°æœ€ä½³
    'yolo_conf': 0.5,        # YOLOç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå»ºè®® 0.5ï¼›èŒƒå›´ 0.4~0.6ï¼Œä½ä¼šæ£€æµ‹å¤šï¼Œé«˜ä¼šæ¼æ£€ï¼‰
    'yolo_iou': 0.45,        # YOLO NMS IOUé˜ˆå€¼ï¼ˆå»ºè®® 0.45ï¼Œ0.4~0.6å‡å¯ï¼Œæ ¹æ®é‡å æƒ…å†µå¾®è°ƒï¼‰
    
    # OCRé…ç½®
    'ocr_backend': 'tesseract',     # 'tesseract', 'paddleocr' æˆ– 'easyocr'
    'ocr_lang': 'chi_sim+eng',      # Tesseractè¯­è¨€ï¼š'chi_sim+eng'=ä¸­è‹±æ–‡, 'eng'=è‹±æ–‡
    'ocr_preprocess': False,        # æ˜¯å¦å¯¹Tesseractä½¿ç”¨é¢„å¤„ç†ï¼ˆé«˜è´¨é‡æˆªå›¾å»ºè®®å…³é—­ï¼‰
    'use_multithread': True,        # æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆä»…Tesseractï¼‰
    'num_workers': 12,              # çº¿ç¨‹æ•°ï¼ˆ96æ ¸æœåŠ¡å™¨ä¼˜åŒ–ä¸º12ï¼‰
    
    # å›¾åƒå¤„ç†
    'padding': 5,                   # è£å‰ªæ¡†çš„è¾¹è·
    'min_text_confidence': 0.5,     # OCRæœ€å°ç½®ä¿¡åº¦
    
    # è¾“å‡ºé…ç½®
    'save_crops': True,             # æ˜¯å¦ä¿å­˜è£å‰ªçš„æ°”æ³¡å›¾ç‰‡
    'save_annotated': True,         # æ˜¯å¦ä¿å­˜æ ‡æ³¨å›¾ç‰‡
    'save_preprocessed': False,     # æ˜¯å¦ä¿å­˜é¢„å¤„ç†åçš„å›¾ç‰‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    'output_dir': 'runs/ocr',       # è¾“å‡ºç›®å½•
    
    # èŠå¤©è®°å½•ä¿å­˜é…ç½®
    'save_chat_records': True,      # æ˜¯å¦ä¿å­˜èŠå¤©è®°å½•ï¼ˆæ”¯æŒæ™ºèƒ½åˆå¹¶ï¼‰
    'session_id': None,             # ä¼šè¯IDï¼ˆNoneåˆ™ä»å›¾ç‰‡åç§°è‡ªåŠ¨ç”Ÿæˆï¼‰
}


# ============= 1. YOLO æ£€æµ‹æ¨¡å— =============

def load_yolo_model(model_path: str) -> YOLO:
    """
    åŠ è½½ YOLO æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        
    Returns:
        YOLOæ¨¡å‹å¯¹è±¡
    """
    if not Path(model_path).exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    print(f"åŠ è½½ YOLO æ¨¡å‹: {model_path}")
    model = YOLO(model_path)
    return model


def detect_bubbles(model: YOLO, image_path: str, conf: float = 0.25, iou: float = 0.45) -> List[Dict]:
    """
    ä½¿ç”¨ YOLO æ£€æµ‹å›¾ç‰‡ä¸­çš„æ°”æ³¡
    
    Args:
        model: YOLOæ¨¡å‹
        image_path: å›¾ç‰‡è·¯å¾„
        conf: ç½®ä¿¡åº¦é˜ˆå€¼
        iou: NMS IOUé˜ˆå€¼
        
    Returns:
        æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
        {
            'bbox': [x1, y1, x2, y2],  # è¾¹ç•Œæ¡†åæ ‡
            'confidence': float,        # ç½®ä¿¡åº¦
            'class_id': int,            # ç±»åˆ«ID
            'class_name': str           # ç±»åˆ«åç§°
        }
    """
    print(f"\næ£€æµ‹æ°”æ³¡: {image_path}")
    
    # è¿›è¡Œæ£€æµ‹
    results = model.predict(
        source=image_path,
        conf=conf,
        iou=iou,
        verbose=False
    )
    
    # è§£æç»“æœ
    detections = []
    if len(results) > 0:
        result = results[0]
        boxes = result.boxes
        
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            confidence = float(box.conf[0].cpu().numpy())
            class_id = int(box.cls[0].cpu().numpy())
            class_name = result.names[class_id]
            
            detections.append({
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'confidence': confidence,
                'class_id': class_id,
                'class_name': class_name
            })
    
    print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªæ°”æ³¡")
    return detections


# ============= 2. å›¾åƒé¢„å¤„ç†æ¨¡å— =============

def crop_bubble(image: np.ndarray, bbox: List[int], padding: int = 5) -> np.ndarray:
    """
    ä»åŸå›¾è£å‰ªæ°”æ³¡åŒºåŸŸ
    
    Args:
        image: åŸå§‹å›¾ç‰‡ï¼ˆnumpyæ•°ç»„ï¼‰
        bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2]
        padding: è¾¹è·ï¼ˆåƒç´ ï¼‰
        
    Returns:
        è£å‰ªåçš„å›¾ç‰‡
    """
    x1, y1, x2, y2 = bbox
    h, w = image.shape[:2]
    
    # æ·»åŠ è¾¹è·ï¼Œä½†ä¸è¶…å‡ºå›¾ç‰‡èŒƒå›´
    x1 = max(0, x1 - padding)
    y1 = max(0, y1 - padding)
    x2 = min(w, x2 + padding)
    y2 = min(h, y2 + padding)
    
    # è£å‰ª
    cropped = image[y1:y2, x1:x2]
    return cropped


def preprocess_for_ocr(image: np.ndarray) -> np.ndarray:
    """
    å›¾åƒé¢„å¤„ç†ï¼Œæé«˜OCRè¯†åˆ«ç‡
    
    Args:
        image: è¾“å…¥å›¾ç‰‡
        
    Returns:
        é¢„å¤„ç†åçš„å›¾ç‰‡
    """
    # è½¬æ¢ä¸ºç°åº¦å›¾
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # å»å™ª
    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
    
    # è‡ªé€‚åº”äºŒå€¼åŒ–ï¼ˆå¯¹ä¸åŒå…‰ç…§æ¡ä»¶æ›´é²æ£’ï¼‰
    binary = cv2.adaptiveThreshold(
        denoised, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        11, 2
    )
    
    # è½»å¾®è†¨èƒ€ï¼Œè¿æ¥æ–­è£‚çš„æ–‡å­—
    kernel = np.ones((2, 2), np.uint8)
    processed = cv2.dilate(binary, kernel, iterations=1)
    
    return processed


def preprocess_for_tesseract(image: np.ndarray) -> np.ndarray:
    """
    ä¸“é—¨ä¸º Tesseract OCR ä¼˜åŒ–çš„é¢„å¤„ç†
    å¯¹äºèŠå¤©æ°”æ³¡åœºæ™¯ï¼Œä¿æŒç®€å•çš„é¢„å¤„ç†æ•ˆæœæœ€å¥½
    
    Args:
        image: è¾“å…¥å›¾ç‰‡ï¼ˆBGRæ ¼å¼ï¼‰
        
    Returns:
        é¢„å¤„ç†åçš„å›¾ç‰‡
    """
    # æ–¹æ³•1: ä¿æŒåŸå›¾ï¼ˆå¯¹äºé«˜è´¨é‡æˆªå›¾æ•ˆæœæœ€å¥½ï¼‰
    # return image
    
    # æ–¹æ³•2: è½»å¾®é¢„å¤„ç†ï¼ˆæ¨èï¼‰
    # è½¬ç°åº¦
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # æ·»åŠ ç™½è‰²è¾¹æ¡†ï¼ˆå¸®åŠ© Tesseract æ›´å¥½åœ°æ£€æµ‹æ–‡æœ¬è¾¹ç•Œï¼‰
    bordered = cv2.copyMakeBorder(gray, 10, 10, 10, 10, cv2.BORDER_CONSTANT, value=255)
    
    # è½»å¾®æ”¾å¤§ï¼ˆå¯¹å°æ–‡å­—æœ‰å¸®åŠ©ï¼‰
    h, w = bordered.shape
    if h < 100 or w < 100:  # å¦‚æœå›¾ç‰‡å¤ªå°ï¼Œæ”¾å¤§2å€
        bordered = cv2.resize(bordered, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
    
    # ä½¿ç”¨åŒè¾¹æ»¤æ³¢å»å™ªï¼ˆä¿ç•™è¾¹ç¼˜ï¼‰
    denoised = cv2.bilateralFilter(bordered, 5, 50, 50)
    
    # è½»å¾®é”åŒ–
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    sharpened = cv2.filter2D(denoised, -1, kernel)
    
    return sharpened


# ============= 3. OCR è¯†åˆ«æ¨¡å— =============

def postprocess_text(text: str) -> str:
    """
    OCR ç»“æœåå¤„ç†ï¼Œä¿®å¤å¸¸è§è¯†åˆ«é”™è¯¯
    
    Args:
        text: åŸå§‹è¯†åˆ«æ–‡æœ¬
        
    Returns:
        ä¿®å¤åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # å¸¸è§çš„ OCR è¯†åˆ«é”™è¯¯ä¿®æ­£
    replacements = {
        # ä¿®å¤å•è¯å¼€å¤´çš„ç«–çº¿ï¼ˆé€šå¸¸æ˜¯å­—æ¯ Iï¼‰
        r'\| ': 'I ',           # "| live" -> "I live"
        r'\|\'': 'I\'',         # "|'m" -> "I'm"
        
        # ä¿®å¤å…¶ä»–å¸¸è§é”™è¯¯
        r'\b0\b': 'O',          # å•ç‹¬çš„ 0 é€šå¸¸æ˜¯å­—æ¯ O
        r'\bl\b': 'I',          # åœ¨æŸäº›ä¸Šä¸‹æ–‡ä¸­ l åº”è¯¥æ˜¯ I
    }
    
    import re
    processed = text
    for pattern, replacement in replacements.items():
        processed = re.sub(pattern, replacement, processed)
    
    return processed


def recognize_text_tesseract(ocr_config: Dict, image: np.ndarray, min_confidence: float = 0.5, 
                            preprocess: bool = True) -> Dict:
    """
    ä½¿ç”¨ Tesseract-OCR è¯†åˆ«æ–‡å­—
    
    Args:
        ocr_config: Tesseracté…ç½®å­—å…¸
        image: è¾“å…¥å›¾ç‰‡
        min_confidence: æœ€å°ç½®ä¿¡åº¦
        preprocess: æ˜¯å¦ä½¿ç”¨ä¸“é—¨çš„é¢„å¤„ç†
        
    Returns:
        è¯†åˆ«ç»“æœå­—å…¸ï¼š
        {
            'text': str,           # å®Œæ•´æ–‡æœ¬
            'lines': List[Dict],   # æ¯è¡Œæ–‡æœ¬è¯¦æƒ…
            'confidence': float    # å¹³å‡ç½®ä¿¡åº¦
        }
    """
    pytesseract = ocr_config['pytesseract']
    lang = ocr_config['lang']
    
    # è·å–è¯¦ç»†çš„OCRæ•°æ®
    try:
        # é¢„å¤„ç†å›¾åƒ
        if preprocess:
            processed_image = preprocess_for_tesseract(image)
        else:
            processed_image = image
        
        # PSM æ¨¡å¼è¯´æ˜:
        # PSM 3: å…¨è‡ªåŠ¨é¡µé¢åˆ†å‰²ï¼ˆé»˜è®¤ï¼‰
        # PSM 4: å‡è®¾æœ‰ä¸€åˆ—ä¸åŒå¤§å°çš„æ–‡æœ¬
        # PSM 6: å‡è®¾æ˜¯å•ä¸ªç»Ÿä¸€çš„æ–‡æœ¬å—
        # PSM 7: å°†å›¾åƒè§†ä¸ºå•è¡Œæ–‡æœ¬
        # PSM 11: ç¨€ç–æ–‡æœ¬ï¼ŒæŒ‰ä»»æ„é¡ºåºæŸ¥æ‰¾å°½å¯èƒ½å¤šçš„æ–‡æœ¬
        # PSM 12: å¸¦ OSD çš„ç¨€ç–æ–‡æœ¬
        
        # å¯¹äºèŠå¤©æ°”æ³¡ï¼Œç»æµ‹è¯• PSM 3ï¼ˆé»˜è®¤ï¼‰æ•ˆæœæœ€å¥½ï¼Œé€‚åˆå¤šè¡Œæ–‡æœ¬
        custom_config = r'--oem 3 --psm 3'
        
        # ä½¿ç”¨ image_to_data è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç½®ä¿¡åº¦ï¼‰
        data = pytesseract.image_to_data(processed_image, lang=lang, config=custom_config, output_type=pytesseract.Output.DICT)
        
        # æå–æ–‡æœ¬å’Œç½®ä¿¡åº¦
        lines = []
        current_line = []
        current_confidences = []
        last_line_num = -1
        
        for i in range(len(data['text'])):
            conf = int(data['conf'][i])
            text = data['text'][i].strip()
            line_num = data['line_num'][i]
            
            # è¿‡æ»¤ç©ºæ–‡æœ¬å’Œä½ç½®ä¿¡åº¦
            if text and conf > 0:
                if line_num != last_line_num and current_line:
                    # æ–°çš„ä¸€è¡Œï¼Œä¿å­˜ä¸Šä¸€è¡Œ
                    line_text = ' '.join(current_line)
                    avg_conf = np.mean(current_confidences) / 100.0
                    
                    if avg_conf >= min_confidence:
                        lines.append({
                            'text': line_text,
                            'confidence': avg_conf,
                            'bbox': None  # Tesseractçš„bboxæ ¼å¼ä¸åŒï¼Œè¿™é‡Œç®€åŒ–
                        })
                    
                    current_line = []
                    current_confidences = []
                
                current_line.append(text)
                current_confidences.append(conf)
                last_line_num = line_num
        
        # å¤„ç†æœ€åä¸€è¡Œ
        if current_line:
            line_text = ' '.join(current_line)
            avg_conf = np.mean(current_confidences) / 100.0
            if avg_conf >= min_confidence:
                lines.append({
                    'text': line_text,
                    'confidence': avg_conf,
                    'bbox': None
                })
        
        # åˆå¹¶æ‰€æœ‰è¡Œ
        full_text = '\n'.join([line['text'] for line in lines])
        avg_confidence = np.mean([line['confidence'] for line in lines]) if lines else 0.0
        
        # åå¤„ç†æ–‡æœ¬ï¼Œä¿®å¤å¸¸è§é”™è¯¯
        full_text = postprocess_text(full_text)
        # åŒæ—¶ä¹Ÿå¤„ç†æ¯ä¸€è¡Œ
        for line in lines:
            line['text'] = postprocess_text(line['text'])
        
        return {
            'text': full_text,
            'lines': lines,
            'confidence': float(avg_confidence)
        }
        
    except Exception as e:
        print(f"  âš ï¸  OCRè¯†åˆ«å‡ºé”™: {e}")
        return {
            'text': '',
            'lines': [],
            'confidence': 0.0
        }


def init_ocr_engine(backend: str = 'tesseract', lang: str = 'chi_sim+eng'):
    """
    åˆå§‹åŒ– OCR å¼•æ“
    
    Args:
        backend: OCRåç«¯ ('tesseract', 'paddleocr' æˆ– 'easyocr')
        lang: è¯­è¨€ä»£ç 
        
    Returns:
        OCRå¼•æ“å¯¹è±¡æˆ–é…ç½®å­—å…¸
    """
    print(f"\nåˆå§‹åŒ– OCR å¼•æ“: {backend} (è¯­è¨€: {lang})")
    
    if backend == 'tesseract':
        try:
            import pytesseract
            # æµ‹è¯•æ˜¯å¦å¯ç”¨
            pytesseract.get_tesseract_version()
            print("âœ… Tesseract-OCR åˆå§‹åŒ–æˆåŠŸ")
            # è¿”å›é…ç½®å­—å…¸
            return {
                'engine': 'tesseract',
                'lang': lang,
                'pytesseract': pytesseract
            }
        except ImportError:
            print("âŒ pytesseract æœªå®‰è£…")
            print("å®‰è£…å‘½ä»¤: pip install pytesseract")
            raise
        except Exception as e:
            print(f"âŒ Tesseract-OCR æœªå®‰è£…æˆ–é…ç½®é”™è¯¯: {e}")
            print("\nå®‰è£… Tesseract-OCR:")
            print("  Ubuntu/Debian: sudo apt-get install tesseract-ocr tesseract-ocr-chi-sim")
            print("  CentOS/RHEL: sudo yum install tesseract tesseract-langpack-chi-sim")
            print("  macOS: brew install tesseract tesseract-lang")
            raise
            
    elif backend == 'paddleocr':
        try:
            from paddleocr import PaddleOCR
            ocr = PaddleOCR(
                use_angle_cls=True,  # ä½¿ç”¨æ–¹å‘åˆ†ç±»å™¨
                lang=lang,
                show_log=False,
                use_gpu=True
            )
            print("âœ… PaddleOCR åˆå§‹åŒ–æˆåŠŸ")
            return ocr
        except ImportError:
            print("âŒ PaddleOCR æœªå®‰è£…")
            print("å®‰è£…å‘½ä»¤: pip install paddleocr paddlepaddle-gpu")
            raise
            
    elif backend == 'easyocr':
        try:
            import easyocr
            lang_map = {'ch': ['ch_sim', 'en'], 'en': ['en']}
            langs = lang_map.get(lang, ['en'])
            ocr = easyocr.Reader(langs, gpu=True)
            print("âœ… EasyOCR åˆå§‹åŒ–æˆåŠŸ")
            return ocr
        except ImportError:
            print("âŒ EasyOCR æœªå®‰è£…")
            print("å®‰è£…å‘½ä»¤: pip install easyocr")
            raise
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„OCRåç«¯: {backend}")


def recognize_text_paddleocr(ocr_engine, image: np.ndarray, min_confidence: float = 0.5) -> Dict:
    """
    ä½¿ç”¨ PaddleOCR è¯†åˆ«æ–‡å­—
    
    Args:
        ocr_engine: PaddleOCRå¯¹è±¡
        image: è¾“å…¥å›¾ç‰‡
        min_confidence: æœ€å°ç½®ä¿¡åº¦
        
    Returns:
        è¯†åˆ«ç»“æœå­—å…¸ï¼š
        {
            'text': str,           # å®Œæ•´æ–‡æœ¬
            'lines': List[Dict],   # æ¯è¡Œæ–‡æœ¬è¯¦æƒ…
            'confidence': float    # å¹³å‡ç½®ä¿¡åº¦
        }
    """
    result = ocr_engine.ocr(image, cls=True)
    
    if result is None or len(result) == 0 or result[0] is None:
        return {
            'text': '',
            'lines': [],
            'confidence': 0.0
        }
    
    lines = []
    text_parts = []
    confidences = []
    
    for line in result[0]:
        box = line[0]  # æ–‡æœ¬æ¡†åæ ‡
        text_info = line[1]  # (æ–‡æœ¬, ç½®ä¿¡åº¦)
        text = text_info[0]
        confidence = text_info[1]
        
        if confidence >= min_confidence:
            lines.append({
                'text': text,
                'confidence': confidence,
                'bbox': box
            })
            text_parts.append(text)
            confidences.append(confidence)
    
    # åˆå¹¶æ‰€æœ‰è¡Œ
    full_text = '\n'.join(text_parts)
    avg_confidence = np.mean(confidences) if confidences else 0.0
    
    return {
        'text': full_text,
        'lines': lines,
        'confidence': float(avg_confidence)
    }


def recognize_text_easyocr(ocr_engine, image: np.ndarray, min_confidence: float = 0.5) -> Dict:
    """
    ä½¿ç”¨ EasyOCR è¯†åˆ«æ–‡å­—
    
    Args:
        ocr_engine: EasyOCR Readerå¯¹è±¡
        image: è¾“å…¥å›¾ç‰‡
        min_confidence: æœ€å°ç½®ä¿¡åº¦
        
    Returns:
        è¯†åˆ«ç»“æœå­—å…¸ï¼ˆæ ¼å¼åŒ PaddleOCRï¼‰
    """
    results = ocr_engine.readtext(image)
    
    lines = []
    text_parts = []
    confidences = []
    
    for detection in results:
        box = detection[0]  # æ–‡æœ¬æ¡†åæ ‡
        text = detection[1]  # æ–‡æœ¬
        confidence = detection[2]  # ç½®ä¿¡åº¦
        
        if confidence >= min_confidence:
            lines.append({
                'text': text,
                'confidence': confidence,
                'bbox': box
            })
            text_parts.append(text)
            confidences.append(confidence)
    
    full_text = '\n'.join(text_parts)
    avg_confidence = np.mean(confidences) if confidences else 0.0
    
    return {
        'text': full_text,
        'lines': lines,
        'confidence': float(avg_confidence)
    }


def recognize_text(ocr_engine, image: np.ndarray, backend: str, 
                  min_confidence: float = 0.5, preprocess: bool = True) -> Dict:
    """
    é€šç”¨ OCR æ–‡å­—è¯†åˆ«æ¥å£ï¼ˆå•å¼ å›¾ç‰‡ï¼‰
    
    Args:
        ocr_engine: OCRå¼•æ“å¯¹è±¡æˆ–é…ç½®å­—å…¸
        image: è¾“å…¥å›¾ç‰‡
        backend: OCRåç«¯ç±»å‹
        min_confidence: æœ€å°ç½®ä¿¡åº¦
        preprocess: æ˜¯å¦é¢„å¤„ç†
        
    Returns:
        è¯†åˆ«ç»“æœå­—å…¸
    """
    # æ ¹æ®åç«¯é€‰æ‹©è¯†åˆ«å‡½æ•°å’Œé¢„å¤„ç†ç­–ç•¥
    if backend == 'tesseract':
        # Tesseract ä½¿ç”¨åŸå›¾æ•ˆæœæ›´å¥½ï¼ˆå·²ç»åœ¨å‡½æ•°å†…éƒ¨å¤„ç†ï¼‰
        return recognize_text_tesseract(ocr_engine, image, min_confidence)
    
    # å¯¹å…¶ä»–OCRå¼•æ“è¿›è¡Œé¢„å¤„ç†
    if preprocess:
        processed_image = preprocess_for_ocr(image)
    else:
        processed_image = image
    
    if backend == 'paddleocr':
        return recognize_text_paddleocr(ocr_engine, processed_image, min_confidence)
    elif backend == 'easyocr':
        return recognize_text_easyocr(ocr_engine, processed_image, min_confidence)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„OCRåç«¯: {backend}")


def recognize_text_batch(ocr_engine, images: List[np.ndarray], backend: str,
                        min_confidence: float = 0.5, preprocess: bool = True,
                        use_multithread: bool = False, num_workers: int = 4) -> List[Dict]:
    """
    æ‰¹é‡ OCR æ–‡å­—è¯†åˆ«æ¥å£
    
    Args:
        ocr_engine: OCRå¼•æ“å¯¹è±¡æˆ–é…ç½®å­—å…¸
        images: è¾“å…¥å›¾ç‰‡åˆ—è¡¨
        backend: OCRåç«¯ç±»å‹
        min_confidence: æœ€å°ç½®ä¿¡åº¦
        preprocess: æ˜¯å¦é¢„å¤„ç†
        use_multithread: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆä»…Tesseractï¼‰
        num_workers: çº¿ç¨‹æ•°
        
    Returns:
        è¯†åˆ«ç»“æœå­—å…¸åˆ—è¡¨
    """
    if len(images) == 0:
        return []
    
    # Tesseract æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
    if backend == 'tesseract':
        if use_multithread and len(images) > 1:
            # å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
            results = [None] * len(images)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨
            
            def process_single(idx_img):
                idx, img = idx_img
                return idx, recognize_text_tesseract(ocr_engine, img, min_confidence, preprocess)
            
            with ThreadPoolExecutor(max_workers=num_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = {executor.submit(process_single, (i, img)): i 
                          for i, img in enumerate(images)}
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    idx, result = future.result()
                    results[idx] = result
            
            return results
        else:
            # å•çº¿ç¨‹å¤„ç†
            results = []
            for img in images:
                result = recognize_text_tesseract(ocr_engine, img, min_confidence, preprocess)
                results.append(result)
            return results
    
    # å¯¹äºå…¶ä»–å¼•æ“ï¼Œé€å¼ å¤„ç†
    elif backend in ['paddleocr', 'easyocr']:
        results = []
        for img in images:
            result = recognize_text(ocr_engine, img, backend, min_confidence, preprocess)
            results.append(result)
        return results
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„OCRåç«¯: {backend}")


# ============= 4. ç»“æœå¯è§†åŒ–æ¨¡å— =============

def draw_results(image: np.ndarray, detections: List[Dict], 
                ocr_results: List[Dict]) -> np.ndarray:
    """
    åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†å’Œè¯†åˆ«æ–‡å­—
    
    Args:
        image: åŸå§‹å›¾ç‰‡
        detections: YOLOæ£€æµ‹ç»“æœ
        ocr_results: OCRè¯†åˆ«ç»“æœ
        
    Returns:
        æ ‡æ³¨åçš„å›¾ç‰‡
    """
    annotated = image.copy()
    
    for i, (detection, ocr_result) in enumerate(zip(detections, ocr_results)):
        bbox = detection['bbox']
        x1, y1, x2, y2 = bbox
        confidence = detection['confidence']
        text = ocr_result['text']
        ocr_conf = ocr_result['confidence']
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        color = (0, 255, 0)  # ç»¿è‰²
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
        
        # å‡†å¤‡æ ‡ç­¾
        label_lines = []
        label_lines.append(f"#{i+1} Conf:{confidence:.2f}")
        
        # æ·»åŠ è¯†åˆ«çš„æ–‡å­—ï¼ˆå¦‚æœæœ‰ï¼‰
        if text:
            # å°†å¤šè¡Œæ–‡æœ¬åˆ†å‰²æ˜¾ç¤º
            text_lines = text.split('\n')
            for line in text_lines[:3]:  # æœ€å¤šæ˜¾ç¤º3è¡Œ
                if len(line) > 20:
                    line = line[:20] + '...'
                label_lines.append(line)
            label_lines.append(f"OCR:{ocr_conf:.2f}")
        else:
            label_lines.append("æ— æ–‡å­—")
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯å’Œæ–‡å­—
        y_offset = y1 - 10
        for line in label_lines:
            # è®¡ç®—æ–‡å­—å¤§å°
            (text_width, text_height), baseline = cv2.getTextSize(
                line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
            )
            
            # ç»˜åˆ¶èƒŒæ™¯
            cv2.rectangle(
                annotated,
                (x1, y_offset - text_height - 5),
                (x1 + text_width + 5, y_offset + baseline),
                color, -1
            )
            
            # ç»˜åˆ¶æ–‡å­—
            cv2.putText(
                annotated, line,
                (x1 + 2, y_offset - 2),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                (255, 255, 255), 1, cv2.LINE_AA
            )
            
            y_offset -= (text_height + baseline + 5)
    
    return annotated


# ============= 5. ç»“æœä¿å­˜æ¨¡å— =============

def classify_message_side(bbox: List[int], image_width: int) -> str:
    """
    æ ¹æ®bboxä½ç½®åˆ¤æ–­æ¶ˆæ¯æ˜¯å·¦ä¾§ï¼ˆå¯¹æ–¹ï¼‰è¿˜æ˜¯å³ä¾§ï¼ˆç”¨æˆ·ï¼‰
    
    Args:
        bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2]
        image_width: å›¾ç‰‡å®½åº¦
        
    Returns:
        'user' æˆ– 'otherparty'
    """
    x1, y1, x2, y2 = bbox
    
    # è®¡ç®—bboxçš„ä¸­å¿ƒç‚¹
    center_x = (x1 + x2) / 2
    
    # æ–¹æ³•1ï¼šç®€å•çš„ä¸­å¿ƒç‚¹åˆ¤æ–­ï¼ˆå¦‚æœä¸­å¿ƒç‚¹åœ¨å³åŠéƒ¨åˆ†ï¼Œåˆ™æ˜¯userï¼‰
    # if center_x > image_width / 2:
    #     return 'user'
    # else:
    #     return 'otherparty'
    
    # æ–¹æ³•2ï¼šæ›´ç²¾ç¡®çš„è¾¹ç•Œè·ç¦»åˆ¤æ–­
    # è®¡ç®—å·¦è¾¹ç•Œåˆ°å·¦ä¾§çš„è·ç¦» vs å³è¾¹ç•Œåˆ°å³ä¾§çš„è·ç¦»
    left_distance = x1  # å·¦è¾¹ç•Œåˆ°å›¾ç‰‡å·¦ä¾§çš„è·ç¦»
    right_distance = image_width - x2  # å³è¾¹ç•Œåˆ°å›¾ç‰‡å³ä¾§çš„è·ç¦»
    
    # å¦‚æœå³è¾¹ç•Œè·ç¦»å³ä¾§æ›´è¿‘ï¼Œè¯´æ˜æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼ˆåœ¨å³è¾¹ï¼‰
    if right_distance < left_distance:
        return 'user'
    else:
        return 'otherparty'


def format_chat_messages(detections: List[Dict], ocr_results: List[Dict], 
                         image_width: int) -> Tuple[List[Dict], str]:
    """
    æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯ï¼Œè¿”å›å¸¦æ ‡ç­¾çš„æ¶ˆæ¯åˆ—è¡¨å’Œæ ¼å¼åŒ–æ–‡æœ¬
    
    Args:
        detections: YOLOæ£€æµ‹ç»“æœ
        ocr_results: OCRè¯†åˆ«ç»“æœ
        image_width: å›¾ç‰‡å®½åº¦
        
    Returns:
        (æ¶ˆæ¯åˆ—è¡¨, æ ¼å¼åŒ–çš„èŠå¤©æ–‡æœ¬)
    """
    chat_messages = []
    chat_lines = []
    
    for i, (detection, ocr_result) in enumerate(zip(detections, ocr_results)):
        text = ocr_result['text'].strip()
        
        # è·³è¿‡æœªè¯†åˆ«åˆ°æ–‡å­—çš„æ°”æ³¡
        if not text:
            continue
        
        # åˆ¤æ–­æ¶ˆæ¯å½’å±
        bbox = detection['bbox']
        side = classify_message_side(bbox, image_width)
        
        # æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        message_info = {
            'id': i + 1,
            'side': side,
            'text': text,
            'bbox': bbox,
            'confidence': ocr_result['confidence']
        }
        chat_messages.append(message_info)
        
        # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬è¡Œ
        chat_lines.append(f"{side}: {text}")
    
    chat_text = '\n'.join(chat_lines)
    return chat_messages, chat_text


def save_results(image_path: str, detections: List[Dict], 
                ocr_results: List[Dict], config: Dict) -> Dict:
    """
    ä¿å­˜æ£€æµ‹å’Œè¯†åˆ«ç»“æœ
    
    Args:
        image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
        detections: YOLOæ£€æµ‹ç»“æœ
        ocr_results: OCRè¯†åˆ«ç»“æœ
        config: é…ç½®å‚æ•°
        
    Returns:
        ä¿å­˜è·¯å¾„ä¿¡æ¯
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    image_name = Path(image_path).stem
    
    result_dir = output_dir / image_name
    result_dir.mkdir(parents=True, exist_ok=True)
    
    # è¯»å–åŸå›¾
    image = cv2.imread(image_path)
    image_height, image_width = image.shape[:2]
    
    saved_files = {
        'result_dir': str(result_dir),
        'crops': [],
        'json': None,
        'annotated': None,
        'chat': None
    }
    
    # ä¿å­˜è£å‰ªçš„æ°”æ³¡å›¾ç‰‡
    if config['save_crops']:
        crops_dir = result_dir / 'crops'
        crops_dir.mkdir(exist_ok=True)
        
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            cropped = crop_bubble(image, bbox, config['padding'])
            
            crop_path = crops_dir / f'bubble_{i+1}.jpg'
            cv2.imwrite(str(crop_path), cropped)
            saved_files['crops'].append(str(crop_path))
    
    # ä¿å­˜é¢„å¤„ç†åçš„å›¾ç‰‡ï¼ˆç”¨äºè°ƒè¯•OCRï¼‰
    if config.get('save_preprocessed', False) and config['ocr_backend'] == 'tesseract':
        preprocessed_dir = result_dir / 'preprocessed'
        preprocessed_dir.mkdir(exist_ok=True)
        
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            cropped = crop_bubble(image, bbox, config['padding'])
            preprocessed = preprocess_for_tesseract(cropped)
            
            prep_path = preprocessed_dir / f'bubble_{i+1}_preprocessed.jpg'
            cv2.imwrite(str(prep_path), preprocessed)
        
        print(f"  âœ“ å·²ä¿å­˜é¢„å¤„ç†å›¾ç‰‡åˆ°: {preprocessed_dir}")
    
    # ä¿å­˜æ ‡æ³¨å›¾ç‰‡
    if config['save_annotated']:
        annotated = draw_results(image, detections, ocr_results)
        annotated_path = result_dir / 'annotated.jpg'
        cv2.imwrite(str(annotated_path), annotated)
        saved_files['annotated'] = str(annotated_path)
    
    # ä¿å­˜JSONç»“æœ
    results_json = {
        'image': image_path,
        'num_bubbles': len(detections),
        'bubbles': []
    }
    
    for i, (detection, ocr_result) in enumerate(zip(detections, ocr_results)):
        bubble_info = {
            'id': i + 1,
            'bbox': detection['bbox'],
            'yolo_confidence': detection['confidence'],
            'class_name': detection['class_name'],
            'text': ocr_result['text'],
            'text_lines': [line['text'] for line in ocr_result['lines']],
            'ocr_confidence': ocr_result['confidence']
        }
        results_json['bubbles'].append(bubble_info)
    
    json_path = result_dir / 'results.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results_json, f, ensure_ascii=False, indent=2)
    saved_files['json'] = str(json_path)
    
    # ä¿å­˜èŠå¤©è®°å½•
    chat_messages, chat_text = format_chat_messages(detections, ocr_results, image_width)
    chat_path = result_dir / 'chat.txt'
    with open(chat_path, 'w', encoding='utf-8') as f:
        f.write(chat_text)
    saved_files['chat'] = str(chat_path)
    
    return saved_files


# ============= 6. ä¸»æµç¨‹æ¨¡å— =============

def process_image(image_path: str, yolo_model: YOLO, ocr_engine, config: Dict, session_id: Optional[str] = None) -> Dict:
    """
    å¤„ç†å•å¼ å›¾ç‰‡ï¼šæ£€æµ‹æ°”æ³¡ + OCRè¯†åˆ« + èŠå¤©è®°å½•ä¿å­˜å’Œåˆå¹¶
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        yolo_model: YOLOæ¨¡å‹
        ocr_engine: OCRå¼•æ“
        config: é…ç½®å‚æ•°
        session_id: ä¼šè¯IDï¼ˆç”¨äºèŠå¤©è®°å½•åˆå¹¶ï¼ŒNoneåˆ™ä»å›¾ç‰‡åç§°è‡ªåŠ¨ç”Ÿæˆï¼‰
        
    Returns:
        å¤„ç†ç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"å¤„ç†å›¾ç‰‡: {image_path}")
    print(f"{'='*60}")
    
    total_start = time.time()
    timing = {}
    
    # 1. YOLOæ£€æµ‹æ°”æ³¡
    step_start = time.time()
    detections = detect_bubbles(
        yolo_model, 
        image_path,
        conf=config['yolo_conf'],
        iou=config['yolo_iou']
    )
    timing['yolo_detection'] = time.time() - step_start
    
    if len(detections) == 0:
        print("æœªæ£€æµ‹åˆ°æ°”æ³¡")
        return {
            'detections': [],
            'ocr_results': [],
            'saved_files': None
        }
    
    # 1.5 æŒ‰ä½ç½®æ’åºï¼ˆä»ä¸Šåˆ°ä¸‹ï¼Œè€ƒè™‘å·¦å³åˆ†å¸ƒï¼‰
    step_start = time.time()
    # å¯¹èŠå¤©æ°”æ³¡è¿›è¡Œæ’åºï¼Œç¡®ä¿ä»ä¸Šåˆ°ä¸‹çš„é¡ºåº
    # ä½¿ç”¨ Y åæ ‡çš„ä¸­å¿ƒç‚¹è¿›è¡Œæ’åºï¼Œè¿™æ ·æ›´å‡†ç¡®
    detections = sorted(detections, key=lambda x: (
        x['bbox'][1],  # ä¸»è¦æŒ‰ Y åæ ‡ï¼ˆä¸Šåˆ°ä¸‹ï¼‰
        x['bbox'][0]   # æ¬¡è¦æŒ‰ X åæ ‡ï¼ˆå·¦åˆ°å³ï¼‰
    ))
    timing['sorting'] = time.time() - step_start
    print(f"âœ… æ£€æµ‹ç»“æœå·²æŒ‰ä»ä¸Šåˆ°ä¸‹é¡ºåºæ’åº (å…± {len(detections)} ä¸ªæ°”æ³¡)")
    
    # è¯»å–å›¾ç‰‡
    image = cv2.imread(image_path)
    
    # 2. æ‰¹é‡è£å‰ªæ‰€æœ‰æ°”æ³¡åŒºåŸŸ
    step_start = time.time()
    print(f"\nè£å‰ªæ°”æ³¡åŒºåŸŸ...")
    cropped_images = []
    for detection in detections:
        bbox = detection['bbox']
        cropped = crop_bubble(image, bbox, config['padding'])
        cropped_images.append(cropped)
    timing['cropping'] = time.time() - step_start
    print(f"âœ… å·²è£å‰ª {len(cropped_images)} ä¸ªæ°”æ³¡åŒºåŸŸ")
    
    # 3. æ‰¹é‡ OCR è¯†åˆ«
    step_start = time.time()
    use_multithread = config.get('use_multithread', False)
    num_workers = config.get('num_workers', 4)
    ocr_preprocess = config.get('ocr_preprocess', True)
    
    if use_multithread and config['ocr_backend'] == 'tesseract':
        print(f"\næ‰¹é‡ OCR è¯†åˆ«ï¼ˆå¤šçº¿ç¨‹ï¼Œ{num_workers} ä¸ªçº¿ç¨‹ï¼‰...")
    else:
        print(f"\næ‰¹é‡ OCR è¯†åˆ«ï¼ˆå•çº¿ç¨‹ï¼‰...")
    
    if config['ocr_backend'] == 'tesseract':
        if ocr_preprocess:
            print(f"  âœ“ ä½¿ç”¨é¢„å¤„ç† + PSM 3 æ¨¡å¼")
        else:
            print(f"  âœ“ ä½¿ç”¨åŸå›¾ + PSM 3 æ¨¡å¼ï¼ˆæ¨èï¼Œæ•ˆæœæœ€ä½³ï¼‰")
    
    ocr_results = recognize_text_batch(
        ocr_engine,
        cropped_images,
        backend=config['ocr_backend'],
        min_confidence=config['min_text_confidence'],
        preprocess=ocr_preprocess,
        use_multithread=use_multithread,
        num_workers=num_workers
    )
    timing['ocr_recognition'] = time.time() - step_start
    
    # 4. æ‰“å°è¯†åˆ«ç»“æœ
    print(f"\nè¯†åˆ«ç»“æœ:")
    for i, ocr_result in enumerate(ocr_results):
        if ocr_result['text']:
            # æ˜¾ç¤ºå®Œæ•´æ–‡æœ¬ï¼Œä¿ç•™æ¢è¡Œç¬¦ï¼ˆç”¨ç©ºæ ¼æ›¿æ¢ä»¥ä¾¿å•è¡Œæ˜¾ç¤ºï¼‰
            full_text = ocr_result['text'].replace('\n', ' ')
            print(f"  æ°”æ³¡ {i+1}/{len(ocr_results)}: '{full_text}' (ç½®ä¿¡åº¦: {ocr_result['confidence']:.2f})")
        else:
            print(f"  æ°”æ³¡ {i+1}/{len(ocr_results)}: æœªè¯†åˆ«åˆ°æ–‡å­—")
    
    # 4.5 æ‰“å°èŠå¤©è®°å½•ï¼ˆæŒ‰å·¦å³åˆ†ç±»ï¼‰
    image_width = image.shape[1]
    chat_messages, chat_text = format_chat_messages(detections, ocr_results, image_width)
    
    print(f"\n{'='*60}")
    print(f"ğŸ’¬ èŠå¤©è®°å½• (å…± {len(chat_messages)} æ¡æœ‰æ•ˆæ¶ˆæ¯):")
    print(f"{'='*60}")
    if chat_text:
        print(chat_text)
    else:
        print("(æ— æœ‰æ•ˆæ¶ˆæ¯)")
    print(f"{'='*60}")
    
    # 4.6 ä¿å­˜å’Œåˆå¹¶èŠå¤©è®°å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    merge_info = None
    if config.get('save_chat_records', True) and chat_messages:
        step_start = time.time()
        
        # ç”Ÿæˆæˆ–ä½¿ç”¨session_id
        if session_id is None:
            session_id = config.get('session_id')
            if session_id is None:
                # ä»å›¾ç‰‡è·¯å¾„ç”Ÿæˆsession_idï¼ˆå»é™¤æ‰©å±•åï¼‰
                session_id = Path(image_path).stem
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼: [{"speaker": "...", "content": "..."}, ...]
        # chat_messages ä¸­çš„ side å­—æ®µéœ€è¦è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_records = []
        for msg in chat_messages:
            # è½¬æ¢ side: 'user' -> 'User', 'otherparty' -> 'OtherParty'
            speaker = msg.get('side', '')
            if speaker == 'user':
                speaker = 'User'
            elif speaker == 'otherparty':
                speaker = 'OtherParty'
            
            standard_records.append({
                "speaker": speaker,
                "content": msg.get('text', '').strip()
            })
        
        # æ™ºèƒ½åˆå¹¶å¹¶ä¿å­˜
        added_count, total_count, operation_type = chat_record_manager.merge_and_save(
            session_id, standard_records
        )
        
        merge_info = {
            'session_id': session_id,
            'added_count': added_count,
            'total_count': total_count,
            'operation_type': operation_type
        }
        
        timing['chat_merge'] = time.time() - step_start
        
        print(f"\nğŸ’¾ èŠå¤©è®°å½•ä¿å­˜:")
        print(f"   ä¼šè¯ID: {session_id}")
        print(f"   æœ¬æ¬¡æ–°å¢: {added_count} æ¡æ¶ˆæ¯")
        print(f"   ç´¯è®¡æ€»æ•°: {total_count} æ¡æ¶ˆæ¯")
        print(f"   æ“ä½œç±»å‹: {operation_type}")
        if 'chat_merge' in timing:
            print(f"   åˆå¹¶è€—æ—¶: {timing['chat_merge']*1000:.2f} ms")
    
    # 5. ä¿å­˜ç»“æœ
    step_start = time.time()
    print(f"\nä¿å­˜ç»“æœ...")
    saved_files = save_results(image_path, detections, ocr_results, config)
    timing['saving'] = time.time() - step_start
    
    timing['total'] = time.time() - total_start
    
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"   æ£€æµ‹åˆ°æ°”æ³¡: {len(detections)} ä¸ª")
    print(f"   è¯†åˆ«åˆ°æ–‡å­—: {sum(1 for r in ocr_results if r['text'])} ä¸ª")
    print(f"   æœ‰æ•ˆæ¶ˆæ¯: {len(chat_messages)} æ¡")
    print(f"   ç»“æœä¿å­˜è‡³: {saved_files['result_dir']}")
    print(f"   èŠå¤©è®°å½•: {saved_files['chat']}")
    
    # æ‰“å°è€—æ—¶ç»Ÿè®¡
    print(f"\nâ±ï¸  è€—æ—¶ç»Ÿè®¡:")
    print(f"   YOLOæ£€æµ‹:    {timing['yolo_detection']*1000:>7.2f} ms")
    print(f"   æ’åº:        {timing['sorting']*1000:>7.2f} ms")
    print(f"   è£å‰ª:        {timing['cropping']*1000:>7.2f} ms")
    print(f"   OCRè¯†åˆ«:     {timing['ocr_recognition']*1000:>7.2f} ms  â­")
    print(f"   ä¿å­˜ç»“æœ:    {timing['saving']*1000:>7.2f} ms")
    if 'chat_merge' in timing:
        print(f"   èŠå¤©åˆå¹¶:    {timing['chat_merge']*1000:>7.2f} ms")
    print(f"   {'â”€'*40}")
    print(f"   æ€»è€—æ—¶:      {timing['total']*1000:>7.2f} ms")
    
    if len(detections) > 0:
        print(f"\n   å¹³å‡æ¯ä¸ªæ°”æ³¡: {timing['ocr_recognition']*1000/len(detections):.2f} ms")
    
    return {
        'detections': detections,
        'ocr_results': ocr_results,
        'saved_files': saved_files,
        'chat_messages': chat_messages,
        'merge_info': merge_info
    }


def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n")
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 15 + "æ°”æ³¡æ£€æµ‹ + OCR è¯†åˆ«ç³»ç»Ÿ" + " " * 15 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    
    # 1. åŠ è½½ YOLO æ¨¡å‹
    try:
        yolo_model = load_yolo_model(CONFIG['yolo_model'])
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nè¯·å…ˆè®­ç»ƒæ¨¡å‹:")
        print("  python train_yolo11n.py")
        return
    
    # 2. åˆå§‹åŒ– OCR å¼•æ“
    try:
        ocr_engine = init_ocr_engine(
            backend=CONFIG['ocr_backend'],
            lang=CONFIG['ocr_lang']
        )
    except Exception as e:
        print(f"\nâŒ OCRå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 3. å¤„ç†å›¾ç‰‡
    # å¯ä»¥æ˜¯å•å¼ å›¾ç‰‡æˆ–ç›®å½•
    # ä½¿ç”¨ç›¸åŒçš„ session_id å¯ä»¥å®ç°å¤šå¼ å›¾ç‰‡çš„èŠå¤©è®°å½•åˆå¹¶
    session_id = CONFIG.get('session_id', 'default_session')  # é»˜è®¤ä¼šè¯ID
    
    test_images = [
        # "/workspace/yolo/image/bumble.jpg",
        # "/workspace/yolo/image/tinder.jpg"
        "/workspace/yolo/image/tinder2.jpg"
        # æ·»åŠ æ›´å¤šæµ‹è¯•å›¾ç‰‡
    ]
    
    results = []
    for i, image_path in enumerate(test_images):
        if Path(image_path).exists():
            # å¯ä»¥ä½¿ç”¨ç›¸åŒçš„session_idæ¥åˆå¹¶å¤šå¼ å›¾ç‰‡çš„è®°å½•
            # æˆ–è€…ä¸ºæ¯å¼ å›¾ç‰‡ä½¿ç”¨ä¸åŒçš„session_id
            result = process_image(image_path, yolo_model, ocr_engine, CONFIG, session_id=session_id)
            results.append(result)
        else:
            print(f"\nâš ï¸  å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
    
    print(f"\n{'='*60}")
    print(f"æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(results)} å¼ å›¾ç‰‡")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    main()
